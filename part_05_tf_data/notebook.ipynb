{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part_04_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iust-deep-learning/tensorflow-2-tutorial/blob/master/part_05_tf_data/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS8LoLU0OQha",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow 2.0 Tutorial: Part #5\n",
        "\n",
        "\n",
        "TensorFlow 2.0 Tutorial by IUST\n",
        "\n",
        "*   Last Update: May 2020\n",
        "*   Official Page: https://github.com/iust-deep-learning/tensorflow-2-tutorial\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSj39l8gcIA8",
        "colab_type": "text"
      },
      "source": [
        "Please run the following cell before going through the rest of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFC1dEyMcFq2",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from pprint import pprint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ3pTDbtePH4",
        "colab_type": "text"
      },
      "source": [
        "### Exercise #1: Simple ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd6aCWATeWx5",
        "colab_type": "text"
      },
      "source": [
        "Convert the following dataset to the one-hot encoding format and vice-versa.\n",
        "\n",
        "a) Normal to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra5ub8bteVKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLASSES = 5\n",
        "\n",
        "ds = tf.data.Dataset.from_tensor_slices(\n",
        "    [0, 2, 1, 3, 3, 4, 1, 3, 3, 4])\n",
        "\n",
        "####################################\n",
        "#   Put your implementation here   #\n",
        "####################################\n",
        "\n",
        "a = list(ds.as_numpy_iterator())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFKWWN9Vj4lD",
        "colab_type": "text"
      },
      "source": [
        "b) one-hot to Normal\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfonpHDjkAn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(\n",
        "    [[1., 0., 0., 0., 0.],\n",
        "       [0., 0., 1., 0., 0.],\n",
        "       [0., 1., 0., 0., 0.],\n",
        "       [0., 0., 0., 1., 0.],\n",
        "       [0., 0., 0., 1., 0.],\n",
        "       [0., 0., 0., 0., 1.],\n",
        "       [0., 1., 0., 0., 0.],\n",
        "       [0., 0., 0., 1., 0.],\n",
        "       [0., 0., 0., 1., 0.],\n",
        "       [0., 0., 0., 0., 1.]], )\n",
        "\n",
        "####################################\n",
        "#   Put your implementation here   #\n",
        "####################################\n",
        "\n",
        "list(ds.as_numpy_iterator())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl1VeMMyoblf",
        "colab_type": "text"
      },
      "source": [
        "### Exercise #2: Seq-to-Seq dataset\n",
        "\n",
        "In this question, we want to create a `tf.data.Dataset` instance for a Sequence-to-Sequence task. The source sequence and the target sequence are both given in a separate stream of the same size. So, every data in the source stream is mapped to a sequence in the target stream at the exact position. \n",
        "\n",
        "1. Create a Dataset instance where each element is a (src_seq, tgt_seq) tuple \n",
        "2. Remove pairs that the length of the source or target sequence is zero.\n",
        "3. Batch and pad the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3bpK-ZvsE63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def get_generator(size):\n",
        "  def gen():\n",
        "    for _ in range(size):\n",
        "      yield np.random.randint(0, 100, size=[np.random.randint(0, 11)])\n",
        "  return gen\n",
        "\n",
        "src_seq_gen = get_generator(10000)\n",
        "tgt_seq_gen = get_generator(10000)\n",
        "\n",
        "src_ds = tf.data.Dataset.from_generator(\n",
        "    src_seq_gen,\n",
        "    output_types=tf.int32, output_shapes=tf.TensorShape([None]))\n",
        "\n",
        "tgt_ds = tf.data.Dataset.from_generator(\n",
        "    tgt_seq_gen,\n",
        "    output_types=tf.int32, output_shapes=tf.TensorShape([None]))\n",
        "\n",
        "ds = #...\n",
        "\n",
        "####################################\n",
        "#   Put your implementation here   #\n",
        "####################################\n",
        "\n",
        "# Hint: You may need:\n",
        "# 1. tf.shape(..), https://www.tensorflow.org/api_docs/python/tf/shape\n",
        "# 2. tf.data.Dataset.padded_batch(...),https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch\n",
        "\n",
        "for s, t in ds.take(10):\n",
        "  print(s)\n",
        "  print(t)\n",
        "  print(\"-------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1DB7g6Dx_Rw",
        "colab_type": "text"
      },
      "source": [
        "### Exercise #3: Language Model Data\n",
        "\n",
        "The Language modeling task, despite being straightforward, requires special data preparation. Dataset for these tasks consists of hundreds of documents, and each document is basically a sequence of words/token. To prepare the dataset for the model, we first concatenate all these documents to form one single giant string. Then, we divide this string to equal-length smaller sequences to create elements of our dataset. Finally, the training batches are constructed by putting several elements together. For instance, suppose that a string 40 elements are created after merging all documents together:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS_meTDpyFdm",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://imgur.com/download/6HjGVpG\"/>\n",
        "</p>\n",
        "\n",
        "In this question, you should implement a data pipeline for a language model that follows the above strategy. That is:\n",
        "\n",
        "1. Read the dataset file. Each line is a single document.\n",
        "Tokenize each document by splitting it using a space.\n",
        "2. Append the `<eod>` token to the end of each document by which the model can learn the concept of documents.\n",
        "3. Concatenate all documents to form a single sequence of words/tokens.\n",
        "4. Divide the aforementioned sequence of tokens to a bunch of 5-element sequences.\n",
        "5. Create a batched dataset with batch_size = 4.\n",
        "\n",
        "Note that your final data pipeline should be something like this `[(batch_1_inputs, batch_1_targets), (batch_2_inputs, batch_2_targets), ..., (batch_n_inputs, batch_n_targets)]`. `targets` in the language modeling task is essentially the shifted inputs (Hint: Create another shifted dataset for targets after step 3. Then, zip the original sequence and the shifted sequence to create a dataset consists of <inputs, targets> pairs.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNWhBH6s6MRx",
        "colab_type": "text"
      },
      "source": [
        "First, run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpyWoZ0C6RwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile dataset.txt\n",
        "d1_w1 d1_w2 d1_w3 d1_w4 d1_w5 d1_w6 d1_w7 d1_w8 d1_w9 d1_w10\n",
        "d2_w1 d2_w2 d2_w3 d2_w4 d2_w5 d2_w6 d2_w7 d2_w8 d2_w9 d2_w10 d2_w11 d2_w12\n",
        "d3_w1 d3_w2 d3_w3 d3_w4 d3_w5 d3_w6 d3_w7 d3_w8 d3_w9 d3_w10 d3_w11 d3_w12 d3_w13 d3_w14 d3_w15\n",
        "d4_w1 d4_w2 d4_w3 d4_w4 d4_w5 d4_w6 d4_w7 d4_w8 d4_w9 d4_w10 d4_w11 d4_w12 d4_w13\n",
        "d5_w1 d5_w2 d5_w3 d5_w4 d5_w5 d5_w6 d5_w7 d5_w8 d5_w9 d5_w10 d5_w11 d5_w12 d5_w13\n",
        "d6_w1 d6_w2 d6_w3 d6_w4 d6_w5 d6_w6 d6_w7 d6_w8 d6_w9\n",
        "d7_w1 d7_w2 d7_w3 d7_w4 d7_w5 d7_w6 d7_w7 d7_w8 d7_w9 d7_w10 d7_w11 d7_w12 d7_w13\n",
        "d8_w1 d8_w2 d8_w3 d8_w4 d8_w5 d8_w6 d8_w7\n",
        "d9_w1 d9_w2 d9_w3 d9_w4 d9_w5 d9_w6 d9_w7 d9_w8 d9_w9 d9_w10 d9_w11 d9_w12\n",
        "d10_w1 d10_w2 d10_w3 d10_w4 d10_w5 d10_w6 d10_w7 d10_w8 d10_w9 d10_w10 d10_w11 d10_w12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AnHxuKa7CPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = tf.data.TextLineDataset('dataset.txt')\n",
        "\n",
        "# Hint: You may need: \n",
        "# 1. tf.strings.split(...) https://www.tensorflow.org/api_docs/python/tf/strings/strip\n",
        "# 2. tf.data.Dataset.flat_map(...) https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map\n",
        "# 3. tf.data.Dataset.zip(...) https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip\n",
        "# 4. tf.data.Dataset.skip(...) https://www.tensorflow.org/api_docs/python/tf/data/Dataset#skip\n",
        "# 5. tf.data.Dataset.batch(...) https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
        "\n",
        "####################################\n",
        "#   Put your implementation here   #\n",
        "####################################\n",
        "\n",
        "ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tXo0f6BgZhYU"
      },
      "source": [
        "## References\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1DT_2hsJ1_gl"
      },
      "source": [
        "\n"
      ]
    }
  ]
}
